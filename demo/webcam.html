<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Motion Estimation Demo</title>
  <script>
  var motionModelName = "motionDense2" // which ML model are we using to infer dense motion from video frames?
  </script>
</head>
<body>
  <div>
    <canvas id=vis width=640 height=480></canvas>
    <!-- <video id=camera width=640 height=480 autoplay></video> -->
  </div>

  <script src="../node_modules/@tensorflow/tfjs/dist/tf.min.js"></script>
  <script>
    // get access to the canvas to draw motion vectors over the video
    var canvas = vis.getContext('2d', { alpha: false })

    class MotionDemo {
      constructor(options) {
        this.canvasTag = document.querySelector(options.canvas)
        this.canvas = this.canvasTag.getContext('2d', {alpha: false})
        this.canvasTag.insertAdjacentHTML('beforebegin', '<video autoplay></video>')
        this.cameraTag = this.canvasTag.previousElementSibling
        this.cameraTag.setAttribute('width', this.canvasTag.getAttribute('width'))
        this.cameraTag.setAttribute('height', this.canvasTag.getAttribute('height'))
        this.pixelWidth = parseInt(this.cameraTag.getAttribute("width"))
        this.pixelHeight = parseInt(this.cameraTag.getAttribute("height"))
        this.motionModelURL = options.modelURL || `../trained-models/trueMotionConv1/model.json`
        this.scale = options.scale || 1.0
        this.strideScale = options.strideScale || 1.0

        this.lastImageTensor = null // persistant to store the last webcam image as a tensorflow tensor
        this.running = false // controls if demo continues running
      }

      async start() {
        this.running = true

        console.log("loading model...")
        this.model = await tf.loadModel(this.motionModelURL)
        this.inputLayer = this.model.getLayer(null, 0)
        if (this.strideScale != 1.0) this.inputLayer.strides = this.inputLayer.strides.map((n)=> Math.round(n * this.strideScale))
        console.log("requesting video stream...")
        const webcamStream = await navigator.mediaDevices.getUserMedia({video: true})
        console.log("ready, starting demo")
        this.cameraTag.srcObject = webcamStream
        requestAnimationFrame(()=> this.process())
      }

      // stop the demo from running
      stop() {
        this.running = false
      }

      // clear out any memory allocated by tensorflow
      dispose() {
        this.stop()
        if (this.lastImageTensor) this.lastImageTensor.dispose()
      }

      // update loop, checks for a new frame, and processes it through the ML model if one has arrived
      async process() {
        if (!this.running) return // check we're still running

        // keep track of if the webcam has pushed a new frame since the last time we read the video buffer
        let isNewFrame = false
        // fetch the video frame and check if it's content has changed
        let thisImageTensor = tf.tidy(()=> {
          // grab a tensor snapshot of the current webcam feed
          let image = tf.fromPixels(this.cameraTag)
          
          // verify we have an old image to compare to
          if (this.lastImageTensor) {
            // check if the frames are the different
            isNewFrame = !tf.all(tf.equal(this.lastImageTensor, image)).get()
          } else {
            // if there's no last frame, then this one's definitely new! the first is always new!
            isNewFrame = true
          }
          // return frame out so it becomes thisImageTensor
          return image
        })
        
        // when a new frame comes in, we can do some processing!
        if (isNewFrame && this.lastImageTensor) {
          // generate interlaced frames in RGBRGB channel format, older pixels first, then newer pixels
          let interlace = tf.tidy(()=> tf.concat([this.lastImageTensor, thisImageTensor], 2).toFloat().div(255).expandDims())
          
          let input
          if (this.scale != 1.0) {
            input = tf.tidy(()=> tf.image.cropAndResize(interlace, [[0,0,1,1]], [0], [Math.round(this.pixelHeight * this.scale), Math.round(this.pixelWidth * this.scale)]))
            interlace.dispose()
          } else {
            input = interlace
          }

          // run bulk prediction job across the whole image
          let predictionsPromise = this.model.predict(input)
          input.dispose()

          // draw the new frame to our visualisation canvas
          await tf.toPixels(thisImageTensor, vis)

          let predictions = await predictionsPromise

          let dotSize = 3
          let indicatorColor = 'red'
          let strides = this.model.getLayer(null, 0).strides.map((x)=> x / this.scale)
          let kernel  = this.model.getLayer(null, 0).kernelSize.map((x)=> x / this.scale)
          for (let cellY = 0; cellY < predictions.shape[1]; cellY++) {
            for (let cellX = 0; cellX < predictions.shape[2]; cellX++) {
              let motionX = predictions.get(0, cellY, cellX, 0)
              let motionY = predictions.get(0, cellY, cellX, 1)

              // let cellPixelBox = this.crop.pixelBoxes[cell]
              // let cellPixelCenter = this.crop.pixelCenters[cell]
              // let strides = this.crop.pixelBoxSizes[cell]
              //let cellPixelBox    = [strides[0] * cellY, strides[1] * cellX, strides[0] * (cellY+1), strides[1] * (cellX+1)]
              let cellPixelCenter = [
                (strides[0] * cellY) + (kernel[0] / 2),
                (strides[1] * cellX) + (kernel[1] / 2)
              ]

              let visLineX = cellPixelCenter[1] + (motionX * (kernel[1] / 2))
              let visLineY = cellPixelCenter[0] + (motionY * (kernel[0] / 2))
              this.canvas.beginPath()
              this.canvas.moveTo(cellPixelCenter[1], cellPixelCenter[0])
              this.canvas.lineTo(visLineX, visLineY)
              this.canvas.closePath()
              this.canvas.strokeStyle = indicatorColor
              this.canvas.stroke()
              this.canvas.beginPath()
              this.canvas.arc(visLineX, visLineY, dotSize / 2, 0, Math.PI * 2)
              this.canvas.closePath()
              this.canvas.fillStyle = indicatorColor
              this.canvas.fill()
            }
          }
          predictions.dispose()

          // clear out memory from old frame and make this frame in to the old frame now we're done
          if (this.lastImageTensor) this.lastImageTensor.dispose()
          this.lastImageTensor = thisImageTensor
        } else {
          // no new frame? either use it to fill the this.lastImageTensor variable, or clear out the memory
          if (!this.lastImageTensor) this.lastImageTensor = thisImageTensor
          else thisImageTensor.dispose()
        }

        // request the browser run this function again next time it gets ready to refresh the screen
        requestAnimationFrame(()=> this.process())
      }
    }

    async function main() {
      window.demo = new MotionDemo({
        canvas: "#vis",
        grid: [24, 18], 
        modelURL: `../trained-models/trueMotionConv4/model.json`,
        scale: 0.25,
        strideScale: 0.5
      })

      await demo.start()
    }

    main()//.catch((err)=> alert(err))
  </script>
</body>
</html>